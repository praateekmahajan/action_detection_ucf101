{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "FOLDER_DATASET = \"data/\"\n",
    "IMAGE_DATASET = \"UCF101_images/\"\n",
    "transformation = transforms.Compose([\n",
    "transforms.RandomCrop(224),\n",
    "transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class DataClass(Dataset):\n",
    "\n",
    "    def __init__(self, data_folder, image_folder, file_name, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.data_folder = data_folder\n",
    "        self.image_folder = image_folder\n",
    "        self.width = 320\n",
    "        self.height = 240\n",
    "        \n",
    "        with open(self.data_folder + file_name) as f:\n",
    "            content = f.readlines()\n",
    "        \n",
    "        self.data = np.asarray([\n",
    "            [i.split(\" \")[0], i.split(\" \")[1], i.split(\" \")[2].split(\"\\n\")[0]] for i in content])\n",
    "\n",
    "    # Generate image files for the given batch of videos\n",
    "    # return batch_size * longest_sequence * channels * height * width\n",
    "    def generatebatch(self, meta_batch):\n",
    "        CHANNELS = 3\n",
    "        folder = self.data_folder + self.image_folder\n",
    "        batch_len = len(meta_batch)\n",
    "\n",
    "        maximum_video_length = meta_batch[:,1].astype(int).max()\n",
    "        \n",
    "        new_batch = np.zeros((batch_len, maximum_video_length, CHANNELS,self.height, self.width))\n",
    "        \n",
    "        for batch_index, file in enumerate(meta_batch):\n",
    "            filename = file[0]\n",
    "            sequence_len = int(file[1])\n",
    "            # generate transformation here if you want to\n",
    "            for i in range(maximum_video_length - sequence_len, maximum_video_length): #pad the beginning\n",
    "                index = i - maximum_video_length + sequence_len\n",
    "                image = cv2.imread(folder + filename + \"_\" + str(index) + \".jpg\")\n",
    "                #apply transformation here if you want to\n",
    "                image = image.transpose(2,0,1)\n",
    "                new_batch[batch_index][i] = image\n",
    "            break\n",
    "            \n",
    "        return new_batch.shape\n",
    "        \n",
    "        \n",
    "    # Get a batch of given batch size\n",
    "    def getbatch(self, batchsize):\n",
    "        batch = np.random.choice(len(self.data), batchsize, replace=False)\n",
    "        batch = self.data[batch]\n",
    "        final_batch = self.generatebatch(batch)\n",
    "        return final_batch\n",
    "    \n",
    "    # Override to give PyTorch size of dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataClass(FOLDER_DATASET, IMAGE_DATASET, \"train1.txt\")\n",
    "validation = DataClass(FOLDER_DATASET, IMAGE_DATASET, \"val1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8912, 625)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 57, 3, 240, 320), (2, 25, 3, 240, 320))"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.getbatch(2), validation.getbatch(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
